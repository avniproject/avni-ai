from typing import Dict, Any

from tests.judge_framework.implementations.conversation.conversation_judge import (
    ConversationJudgeStrategyWrapper,
)
from tests.judge_framework.interfaces.result_models import (
    TestConfiguration,
    EvaluationResult,
)


class RulesGenerationJudgeWrapper(ConversationJudgeStrategyWrapper):
    def __init__(self, config: TestConfiguration):
        super().__init__(config)

    def evaluate(
        self, test_input: Dict[str, Any], test_output: Dict[str, Any]
    ) -> EvaluationResult:
        return super().evaluate(test_input, test_output)

    @staticmethod
    def _create_evaluation_prompt(
        test_case: Dict[str, Any], actual_response: str
    ) -> str:
        expected_rule = test_case.get("reference_rule", "")
        rule_context = test_case.get("reference_context", {})
        rule_request = test_case.get("rule_request", "")

        evaluation_prompt = f"""
You are evaluating a visit schedule rule generated by an AI assistant for the Avni platform.

**Context:**
Form Type: {rule_context.get("formType", "Unknown")}
Encounter Type: {rule_context.get("encounterType", "Unknown")}
Available Concepts: {rule_context.get("concepts", [])}
Available Encounter Types: {rule_context.get("encounterTypes", [])}

**User Request:**
{rule_request}

**Generated Rule:**
```javascript
{actual_response}
```

**Reference Rule:**
```javascript
{expected_rule}
```

Please evaluate the generated rule on these metrics (score 0-100):

1. **Rule Correctness (25 points)**: Does the rule correctly implement the requested scheduling logic?
2. **Timing Accuracy (25 points)**: Are the dates, intervals, and scheduling conditions accurate?
3. **Code Quality (25 points)**: Is the JavaScript code well-structured, follows patterns, and handles edge cases?
4. **User Communication (25 points)**: Does the response explain the rule clearly and address the user's needs?

**Scoring Guidelines:**
- 90-100: Excellent, production-ready rule
- 80-89: Good rule with minor issues
- 70-79: Acceptable rule with some problems
- 60-69: Poor rule with significant issues
- Below 60: Unacceptable rule

**Analysis Format:**
```json
{{
  "rule_correctness": {{
    "score": <0-100>,
    "reasoning": "<detailed analysis of correctness>",
    "issues": ["<list of correctness issues>"]
  }},
  "timing_accuracy": {{
    "score": <0-100>,
    "reasoning": "<detailed analysis of timing logic>",
    "issues": ["<list of timing issues>"]
  }},
  "code_quality": {{
    "score": <0-100>,
    "reasoning": "<detailed analysis of code structure>",
    "issues": ["<list of code quality issues>"]
  }},
  "user_communication": {{
    "score": <0-100>,
    "reasoning": "<detailed analysis of communication>",
    "issues": ["<list of communication issues>"]
  }},
  "overall_assessment": {{
    "total_score": <average of all scores>,
    "strengths": ["<list of strengths>"],
    "weaknesses": ["<list of weaknesses>"],
    "recommendations": ["<list of recommendations>"]
  }}
}}
```
"""
        return evaluation_prompt

    def get_judge_metadata(self) -> Dict[str, Any]:
        """Get metadata about the rules generation judge"""
        return {
            "judge_type": "RulesGenerationJudgeWrapper",
            "evaluation_focus": "visit_schedule_rules",
            "supported_metrics": [
                "rule_correctness",
                "timing_accuracy",
                "code_quality",
                "user_communication",
            ],
            "javascript_aware": True,
            "avni_platform_specific": True,
        }
